<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.junzhe.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="前言 这是 cs336 课程 assignment1 的第一个部分，要求实现一个 byte-level byte-pair encoding (BPE) 分词器 Byte-Level BPE 是 BPE 的一个变体，其最关键的区别在于：它的最基础单元是字节（Byte），而不是 Unicode 字符。一般我们说到的 BPE 是基于字符的 BPE，如果指明了是 Byte-Level BPE，那么它的基">
<meta property="og:type" content="article">
<meta property="og:title" content="cs336-bpe 分词器">
<meta property="og:url" content="https://www.junzhe.top/posts/cs336-bpe-tokenizer.html">
<meta property="og:site_name" content="濬哲的博客">
<meta property="og:description" content="前言 这是 cs336 课程 assignment1 的第一个部分，要求实现一个 byte-level byte-pair encoding (BPE) 分词器 Byte-Level BPE 是 BPE 的一个变体，其最关键的区别在于：它的最基础单元是字节（Byte），而不是 Unicode 字符。一般我们说到的 BPE 是基于字符的 BPE，如果指明了是 Byte-Level BPE，那么它的基">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-09-10T03:19:56.000Z">
<meta property="article:modified_time" content="2025-09-10T08:00:20.189Z">
<meta property="article:author" content="濬哲">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://www.junzhe.top/posts/cs336-bpe-tokenizer.html">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.junzhe.top/posts/cs336-bpe-tokenizer.html","path":"posts/cs336-bpe-tokenizer.html","title":"cs336-bpe 分词器"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>cs336-bpe 分词器 | 濬哲的博客</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="濬哲的博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">濬哲的博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">人生如棋，落子无悔</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#unicode-standard"><span class="nav-number">2.</span> <span class="nav-text">Unicode Standard</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#unicode-encoding"><span class="nav-number">3.</span> <span class="nav-text">Unicode Encoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#subword-tokenization"><span class="nav-number">4.</span> <span class="nav-text">Subword Tokenization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bpe-tokenizer-training"><span class="nav-number">5.</span> <span class="nav-text">BPE Tokenizer Training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="nav-number">5.1.</span> <span class="nav-text">实现细节</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pre-tokenization"><span class="nav-number">5.1.1.</span> <span class="nav-text">Pre-tokenization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bpe-merges"><span class="nav-number">5.1.2.</span> <span class="nav-text">BPE Merges</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bpe-tokenizer-encoding-and-decoding"><span class="nav-number">6.</span> <span class="nav-text">BPE Tokenizer Encoding and
Decoding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#encoding"><span class="nav-number">6.1.</span> <span class="nav-text">Encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decoding"><span class="nav-number">6.2.</span> <span class="nav-text">Decoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E7%9A%84bpe-tokenizer"><span class="nav-number">6.3.</span> <span class="nav-text">完整的 BPE Tokenizer</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="濬哲"
      src="/images/head.jpg">
  <p class="site-author-name" itemprop="name">濬哲</p>
  <div class="site-description" itemprop="description">技术沉思与人间烟火</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/DreamKongcheng" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;DreamKongcheng" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3377965639@qq.com" title="E-Mail → mailto:3377965639@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://dreamkongcheng.github.io/about/" title="Wechat → https:&#x2F;&#x2F;dreamkongcheng.github.io&#x2F;about&#x2F;" rel="noopener me" target="_blank"><i class="fab fa-weixin fa-fw"></i>Wechat</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.junzhe.top/posts/cs336-bpe-tokenizer.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.jpg">
      <meta itemprop="name" content="濬哲">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="濬哲的博客">
      <meta itemprop="description" content="技术沉思与人间烟火">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="cs336-bpe 分词器 | 濬哲的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          cs336-bpe 分词器
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-09-10 11:19:56 / 修改时间：16:00:20" itemprop="dateCreated datePublished" datetime="2025-09-10T11:19:56+08:00">2025-09-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/cs336/" itemprop="url" rel="index"><span itemprop="name">cs336</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2>
<p>这是 cs336 课程 assignment1 的第一个部分，要求实现一个 byte-level
byte-pair encoding (BPE) 分词器</p>
<p>Byte-Level BPE
是 BPE 的一个变体，其最关键的区别在于：它的最基础单元是字节（Byte），而不是 Unicode 字符。一般我们说到的 BPE 是基于字符的 BPE，如果指明了是 Byte-Level
BPE，那么它的基础单元就是字节。</p>
<p>基于字节的 BPE 的词表只有 256 个字节（0-255），几乎不会出现 oov 的问题。而基于字符的 BPE 的词表大小则取决于训练语料中出现的字符数量，通常会远大于 256，也会出现 oov 的问题。</p>
<span id="more"></span>
<h2 id="unicode-standard">Unicode Standard</h2>
<p>在了解 BPE 之前，我们需要先了解 Unicode Standard。Unicode
Standard 是一个字符编码标准，它为每个字符分配了一个唯一的代码点（code
point）。代码点是一个整数，通常用十六进制表示，例如 U + 0041 表示字符’A’。</p>
<h2 id="unicode-encoding">Unicode Encoding</h2>
<p>Unicode 编码是将 Unicode 代码点转换为字节序列的过程。常见的 Unicode 编码有 UTF-8、UTF-16 和 UTF-32。UTF-8 是最常用的 Unicode 编码，它使用 1 到 4 个字节表示一个字符。UTF-8 的优点是兼容 ASCII 编码，并且对于常用字符使用较少的字节表示。</p>
<p>cs336 作业中的一段介绍： &gt; While the Unicode standard defines a
mapping from characters to code points (integers), it’s impractical to
train tokenizers directly on Unicode codepoints, since the vocabulary
would be prohibitively large (around 150K items) and sparse (since many
characters are quite rare). Instead, we’ll use a Unicode encoding, which
converts a Unicode character into a sequence of bytes</p>
<p>To encode a Unicode string into UTF-8, we can use the
<code>encode()</code> function in Python </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_string = <span class="string">"hello! こんにちは!"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>utf8_encoded = test_string.encode(<span class="string">"utf-8"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(utf8_encoded)</span><br><span class="line"><span class="string">b'hello! \xe3\x81\x93\xe3\x82\x93\xe3\x81\xab\xe3\x81\xa1\xe3\x81\xaf!'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">type</span>(utf8_encoded))</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">'bytes'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Get the byte values for the encoded string (integers from 0 to 255).</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(utf8_encoded)</span><br><span class="line">[<span class="number">104</span>, <span class="number">101</span>, <span class="number">108</span>, <span class="number">108</span>, <span class="number">111</span>, <span class="number">33</span>, <span class="number">32</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">147</span>, <span class="number">227</span>, <span class="number">130</span>, <span class="number">147</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">171</span>, <span class="number">227</span>, <span class="number">129</span>,</span><br><span class="line"><span class="number">161</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">175</span>, <span class="number">33</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># One byte does not necessarily correspond to one Unicode character!</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">len</span>(test_string))</span><br><span class="line"><span class="number">13</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">len</span>(utf8_encoded))</span><br><span class="line"><span class="number">23</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(utf8_encoded.decode(<span class="string">"utf-8"</span>))</span><br><span class="line">hello! こんにちは!</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>By converting our Unicode codepoints into a sequence of bytes (e.g.,
via the UTF-8 encoding), we are essentially taking a sequence of
codepoints (integers in the range 0 to 154,997) and transforming it into
a sequence of byte values (integers in the range 0 to 255).</p>
<p>具体的编码规则不进行展开了</p>
<h2 id="subword-tokenization">Subword Tokenization</h2>
<p>While byte-level tokenization can alleviate the out-of-vocabulary
issues faced by word-level tokenizers, tokenizing text into bytes
results in extremely long input sequences. 虽然 byte-level
tokenization 可以解决 word-level
tokenizers 导致的 oov 的问题，但是这样会导致输入序列非常长。</p>
<p>Subword tokenization is a midpoint between word-level tokenizers and
byte-level tokenizers.</p>
<p>我们会把那些经常出现在一起的字节对（byte
pair）合并成一个新的子词（subword）。这样，我们就可以在保持较小词表的同时，减少输入序列的长度。</p>
<p>For example, if the byte sequence b’the’ often occurs in our raw text
training data, assigning it an entry in the vocabulary would reduce this
3-token sequence to a single token.</p>
<h2 id="bpe-tokenizer-training">BPE Tokenizer Training</h2>
<p>BPE 分词器的训练主要分为 3 步</p>
<ol type="1">
<li><strong>Vocabulary initialization</strong>:
词表是一个从 bytestring 到 int 的一一映射。由于我们采用的是 byte-level
BPE，所以词表的初始大小就是 256</li>
<li><strong>Pre-tokenization</strong>:
<ul>
<li>理论上，有了初始的词表之后，我们就可以直接遍历所有的字节对，然后把出现频率最高的字节对合并成一个新的子词，加入到词表中。但是这样做效率非常低，因为每一次我们都要遍历整一个语料库。另外直接合并字节可能会产生跨单词边界的 token，或者创建出仅在标点符号上有差异的冗余 token（如 dog 和 dog! 被视为完全不同的 token），这浪费了词汇表空间且不符合语言规律。</li>
<li>加上了预分词之后，我们可以先把语料库分成一个个单词，然后在每个单词内部进行 BPE 合并。这样做的好处是不需要再去原始的字节流中统计频率，而是可以直接统计这些 “预 token” 中各个字节对的频率。另外，预分词也能避免跨单词边界的 token（一般都不跨单词边界合并）。</li>
<li>例子：如果单词” text” 在语料中出现了 10 次，那么当我们统计字节’t’和’e’在” text” 内部相邻的频率时，我们可以直接为这个词对 (t,
e) 的频率加上 10，而不是在原始语料中扫描 10 次。这大大提升了统计效率。</li>
</ul></li>
<li><strong>Compute BPE merges</strong>:
现在已经把输入变成了 pre-tokns 并且每一个 pre-token 都是一个 utf-8 编码的 bytestring。接下来我们就可以统计这些 pre-token 中各个字节对的频率，然后把出现频率最高的字节对合并成一个新的子词，加入到词表中。并且接下来要把所有包含这个字节对的 pre-token 都进行更新（合并这个字节对）。重复这个过程，直到词表达到指定的大小。作业中要求如果有多个字节对频率相同，那么就选择字典序最大的那个字节对进行合并。</li>
</ol>
<p>另外需要注意 special tokens 的处理。special
tokens 是一些特殊的 token，比如 [PAD]、[UNK]、[CLS]、[SEP]、&lt;|endoftext|&gt; 等。这些 token 在训练 BPE 分词器时需要被加入到词表中，并且不能被修改。</p>
<h3 id="实现细节">实现细节</h3>
<h4 id="pre-tokenization">Pre-tokenization</h4>
<p>首先是预分词部分，这一部分是可以进行并行的。实验要求中说 pre-tokenize 是主要的性能瓶颈，而这一部分瓶颈是可以通过并行解决，思路就是把语料库分成多个 chunk，然后每个 chunk 交给一个线程进行预分词，最后把所有线程的结果合并起来。</p>
<p>然后注意这一部分是不能包含 special tokens 的。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pre_tokenize</span>(<span class="params">text: <span class="built_in">str</span>, special_tokens: <span class="built_in">list</span>[<span class="built_in">str</span>] | <span class="literal">None</span> = <span class="literal">None</span>, preserve_special_tokens: <span class="built_in">bool</span> = <span class="literal">True</span></span>) -&gt; <span class="built_in">list</span>[<span class="built_in">bytes</span>]:</span><br><span class="line">    docs = split_by_special_tokens(text, special_tokens)</span><br><span class="line">    <span class="comment"># print("\ndocs: ", docs) # ['Héllò hôw ', '&lt;|endoftext|&gt;', '', '&lt;|endoftext|&gt;', ' are ü? 🙃', '&lt;|endoftext|&gt;', '']</span></span><br><span class="line">    PAT = PAT = <span class="string">r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""</span></span><br><span class="line">    token_bytes_list = []</span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> docs:</span><br><span class="line">        <span class="keyword">if</span> special_tokens <span class="keyword">and</span> doc <span class="keyword">in</span> special_tokens:</span><br><span class="line">            <span class="keyword">if</span> preserve_special_tokens:</span><br><span class="line">                special_token_bytes = doc.encode(<span class="string">"utf-8"</span>)</span><br><span class="line">                token_bytes_list.extend([special_token_bytes])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tokens = re.findall(PAT, doc) <span class="comment"># ['the', ' cat', ' ate']</span></span><br><span class="line">            token_bytes = [t.encode(<span class="string">"utf-8"</span>) <span class="keyword">for</span> t <span class="keyword">in</span> tokens] <span class="comment"># [b'the', b' cat', b' ate'].</span></span><br><span class="line">            token_bytes_list.extend(token_bytes)</span><br><span class="line">    <span class="comment"># print("token_bytes_list:", token_bytes_list)</span></span><br><span class="line">    <span class="keyword">return</span> token_bytes_list</span><br></pre></td></tr></tbody></table></figure>
<h4 id="bpe-merges">BPE Merges</h4>
<p>这一部分是整个 BPE 训练的核心部分，基本的算法也很直观。首先构造一个 word_cnt 字典，key 是 pre-token 的 bytestring，value 是这个 pre-token 在语料中出现的次数。然后统计所有 pre-token 中相邻字节对的频率，选择频率最高的字节对进行合并，更新 word_cnt，更新 merges 列表，更新 vocab。重复这个过程，直到词表达到指定的大小。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">merges = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">    pair_cnt = count_pair(word_cnt)</span><br><span class="line">    max_pair = MAX(pair_cnt)</span><br><span class="line">    merges.append(max_pair)</span><br><span class="line">    vocab[base_vocab_size + i] = max_pair  <span class="comment"># 直接赋值</span></span><br><span class="line"></span><br><span class="line">    new_word_cnt = {}</span><br><span class="line">    <span class="keyword">for</span> word_bytes, cnt <span class="keyword">in</span> word_cnt.items():</span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        new_word_bytes = []</span><br><span class="line">        <span class="keyword">while</span> j &lt; <span class="built_in">len</span>(word_bytes):</span><br><span class="line">            <span class="keyword">if</span> j &lt; <span class="built_in">len</span>(word_bytes) - <span class="number">1</span> <span class="keyword">and</span> (word_bytes[j], word_bytes[j+<span class="number">1</span>]) == max_pair:</span><br><span class="line">                new_word_bytes.append(max_pair[<span class="number">0</span>] + max_pair[<span class="number">1</span>])</span><br><span class="line">                j += <span class="number">2</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_word_bytes.append(word_bytes[j])</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">        new_word_bytes = <span class="built_in">tuple</span>(new_word_bytes)</span><br><span class="line">        new_word_cnt[new_word_bytes] = new_word_cnt.get(new_word_bytes, <span class="number">0</span>) + cnt</span><br><span class="line">    word_cnt = new_word_cnt  <span class="comment"># 更新word_cnt</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> vocab, merges</span><br></pre></td></tr></tbody></table></figure>
<p>当然，上面的代码是没法过测试的，因为时间复杂度很高，有如下几个地方可以进行优化
1. 找到频率最高的字节对时，可以使用堆（heap）来优化 2.
更新 word_cnt 时，可以只更新包含 max_pair 的 pre-token，而不是所有的 pre-token，这里可以使用倒排索引来记录包含每个字节对的 pre-token 列表</p>
<p>总之，优化的思路就是使用更加高效的数据结构以及减少不必要的计算，完整的优化后代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bpe</span>(<span class="params"></span></span><br><span class="line"><span class="params">    input_path: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    vocab_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    special_tokens: <span class="built_in">list</span>[<span class="built_in">str</span>]</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="built_in">tuple</span>[<span class="built_in">dict</span>[<span class="built_in">int</span>, <span class="built_in">bytes</span>], <span class="built_in">list</span>[<span class="built_in">tuple</span>[<span class="built_in">bytes</span>, <span class="built_in">bytes</span>]]]:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(input_path, <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        num_processes = <span class="number">8</span></span><br><span class="line">        boundaries = find_chunk_boundaries(f, num_processes, <span class="string">b"&lt;|endoftext|&gt;"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The following is a serial implementation, but you can parallelize this</span></span><br><span class="line">        <span class="comment"># by sending each start/end pair to a set of processes.</span></span><br><span class="line">        chunks = []</span><br><span class="line">        <span class="keyword">for</span> start, end <span class="keyword">in</span> <span class="built_in">zip</span>(boundaries[:-<span class="number">1</span>], boundaries[<span class="number">1</span>:]):</span><br><span class="line">            f.seek(start)</span><br><span class="line">            chunk = f.read(end - start).decode(<span class="string">"utf-8"</span>, errors=<span class="string">"ignore"</span>)</span><br><span class="line">            <span class="comment"># Run pre-tokenization on your chunk and store the counts for each pre-token</span></span><br><span class="line">            chunks.append(chunk)</span><br><span class="line"></span><br><span class="line">        args = [(chunk, special_tokens) <span class="keyword">for</span> chunk <span class="keyword">in</span> chunks]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> multiprocessing.Pool(processes=num_processes) <span class="keyword">as</span> pool:</span><br><span class="line">            results = pool.starmap(process_chunk, args)</span><br><span class="line">            </span><br><span class="line">        word_cnt = Counter()</span><br><span class="line">        <span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">            word_cnt.update(result)</span><br><span class="line">    </span><br><span class="line">        vocab = get_basic_vocab(special_tokens)</span><br><span class="line">        base_vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line">        num_merges = vocab_size - base_vocab_size</span><br><span class="line"></span><br><span class="line">        pair_cnt, pair2word_bytes = count_pair(word_cnt)</span><br><span class="line"></span><br><span class="line">        merges = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> i  % <span class="number">100</span> == <span class="number">0</span>:  <span class="comment"># 每100轮重建一次堆，避免堆变得太大</span></span><br><span class="line">                heap = MaxHeap()</span><br><span class="line">                <span class="keyword">for</span> pair, cnt <span class="keyword">in</span> pair_cnt.items():</span><br><span class="line">                    heap.push((cnt, pair))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 懒惰删除</span></span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                cnt, pair = heap.pop()</span><br><span class="line">                <span class="comment"># print(cnt, pair)</span></span><br><span class="line">                <span class="keyword">if</span> pair <span class="keyword">in</span> pair_cnt <span class="keyword">and</span> pair_cnt[pair] == cnt:</span><br><span class="line">                    max_pair = pair</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="comment"># 否则丢弃，继续弹出</span></span><br><span class="line">            </span><br><span class="line">            merges.append(max_pair)</span><br><span class="line">            vocab[base_vocab_size + i] = max_pair[<span class="number">0</span>] + max_pair[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新word_cnt</span></span><br><span class="line">            <span class="comment"># 只需要改动那些max_pair合并影响到的</span></span><br><span class="line">            affected_word_bytes = pair2word_bytes[max_pair] <span class="comment"># set</span></span><br><span class="line">            pair2word_bytes.pop(max_pair)</span><br><span class="line">            affected_pairs = <span class="built_in">set</span>()</span><br><span class="line">            <span class="keyword">for</span> word_bytes <span class="keyword">in</span> affected_word_bytes:</span><br><span class="line">                cnt = word_cnt[word_bytes]</span><br><span class="line">                word_cnt.pop(word_bytes)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(word_bytes[:-<span class="number">1</span>], word_bytes[<span class="number">1</span>:]):</span><br><span class="line">                    pair_cnt[pair] -= cnt</span><br><span class="line">                    <span class="keyword">if</span> pair_cnt[pair] == <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">del</span> pair_cnt[pair]</span><br><span class="line">                    pair2word_bytes[pair].discard(word_bytes)</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> pair2word_bytes[pair]:</span><br><span class="line">                        <span class="keyword">del</span> pair2word_bytes[pair]</span><br><span class="line"></span><br><span class="line">                    affected_pairs.add(pair)  <span class="comment"># 因为有些pair及时变少了，多轮以后可能也是最大的，所以这里必须要追踪</span></span><br><span class="line"></span><br><span class="line">                j = <span class="number">0</span></span><br><span class="line">                new_word_bytes = []</span><br><span class="line">                <span class="keyword">while</span> j &lt; <span class="built_in">len</span>(word_bytes):</span><br><span class="line">                    <span class="keyword">if</span> j &lt; <span class="built_in">len</span>(word_bytes) - <span class="number">1</span> <span class="keyword">and</span> (word_bytes[j], word_bytes[j+<span class="number">1</span>]) == max_pair: <span class="comment"># 遇到要合并的情况</span></span><br><span class="line">                        new_word_bytes.append(max_pair[<span class="number">0</span>] + max_pair[<span class="number">1</span>])</span><br><span class="line">                        j += <span class="number">2</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        new_word_bytes.append(word_bytes[j])</span><br><span class="line">                        j += <span class="number">1</span></span><br><span class="line">                new_word_bytes = <span class="built_in">tuple</span>(new_word_bytes)</span><br><span class="line">                word_cnt[new_word_bytes] += cnt</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> i, pair <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(new_word_bytes[:-<span class="number">1</span>], new_word_bytes[<span class="number">1</span>:])):</span><br><span class="line">                    pair_cnt[pair] += cnt</span><br><span class="line">                    pair2word_bytes[pair].add(new_word_bytes)</span><br><span class="line">                    affected_pairs.add(pair)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> pair <span class="keyword">in</span> affected_pairs:</span><br><span class="line">                <span class="keyword">if</span> pair <span class="keyword">in</span> pair_cnt:</span><br><span class="line">                    heap.push((pair_cnt[pair], pair))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab, merges</span><br></pre></td></tr></tbody></table></figure>
<p>这段代码跑完第一个测试点只需要 0.3s，远超过 1.5s 的要求</p>
<h2 id="bpe-tokenizer-encoding-and-decoding">BPE Tokenizer Encoding and
Decoding</h2>
<p>BPE 分词器的编码和解码相对简单，主要是根据训练好的词表和 merges 进行操作。</p>
<h3 id="encoding">Encoding</h3>
<ol type="1">
<li><strong>Pre-tokenization</strong>:
这一部分和训练时的预分词是一样的，都是把输入文本分成一个个 pre-token。</li>
<li><strong>Apply the merges</strong>:
对每个 pre-token，先拆成一个个的字节，然后<strong>根据 merges 列表的顺序</strong>进行合并，直到不能再合并为止。最后把所有的 token 的 id 拼接起来，作为最终的编码结果。
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 必须按照词表的顺序</span></span><br><span class="line"><span class="keyword">for</span> merge <span class="keyword">in</span> <span class="variable language_">self</span>.merges:</span><br><span class="line">    new_token_bytes = []</span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(merge)</span></span><br><span class="line">    <span class="keyword">while</span> j &lt; <span class="built_in">len</span>(token_bytes):</span><br><span class="line">        <span class="keyword">if</span> j &lt; <span class="built_in">len</span>(token_bytes) - <span class="number">1</span> <span class="keyword">and</span> (token_bytes[j], token_bytes[j+<span class="number">1</span>]) == merge:</span><br><span class="line">            new_token_bytes.append(token_bytes[j] + token_bytes[j+<span class="number">1</span>])</span><br><span class="line">            j += <span class="number">2</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_token_bytes.append(token_bytes[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># print(new_token_bytes)</span></span><br><span class="line">    token_bytes = new_token_bytes</span><br></pre></td></tr></tbody></table></figure>
由此可见，每一个 token 都要把 merge 表从头到尾遍历一遍，时间复杂度是 O (n * m)</li>
</ol>
<p>另外需要注意，special tokens 需要被保留</p>
<h3 id="decoding">Decoding</h3>
<p>解码相对简单，就是把 token
id 转换成 bytes（通过 vocab），然后拼接成一个 bytestring</p>
<h3 id="完整的bpe-tokenizer">完整的 BPE Tokenizer</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BPETokenizer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, </span></span><br><span class="line"><span class="params">        vocab:  <span class="built_in">dict</span>[<span class="built_in">int</span>, <span class="built_in">bytes</span>],</span></span><br><span class="line"><span class="params">        merges:  <span class="built_in">list</span>[<span class="built_in">tuple</span>[<span class="built_in">bytes</span>, <span class="built_in">bytes</span>]],</span></span><br><span class="line"><span class="params">        special_tokens: <span class="built_in">list</span>[<span class="built_in">str</span>] | <span class="literal">None</span> = <span class="literal">None</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="variable language_">self</span>.vocab = vocab</span><br><span class="line">        <span class="variable language_">self</span>.merges = merges</span><br><span class="line">        <span class="variable language_">self</span>.special_tokens = special_tokens</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_files</span>(<span class="params"></span></span><br><span class="line"><span class="params">        cls, </span></span><br><span class="line"><span class="params">        vocab_filepath: <span class="built_in">str</span>, </span></span><br><span class="line"><span class="params">        merges_filepath: <span class="built_in">str</span>, </span></span><br><span class="line"><span class="params">        special_tokens: <span class="built_in">list</span>[<span class="built_in">str</span>] | <span class="literal">None</span> = <span class="literal">None</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(vocab_filepath) <span class="keyword">as</span> vocab_f:</span><br><span class="line">            vocab = json.load(vocab_f)</span><br><span class="line">        merges = []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(merges_filepath) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                cleaned_line = line.rstrip()</span><br><span class="line">                <span class="keyword">if</span> cleaned_line <span class="keyword">and</span> <span class="built_in">len</span>(cleaned_line.split(<span class="string">" "</span>)) == <span class="number">2</span>:</span><br><span class="line">                    merges.append(<span class="built_in">tuple</span>(cleaned_line.split(<span class="string">" "</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> special_tokens:</span><br><span class="line">            <span class="keyword">for</span> special_token <span class="keyword">in</span> special_tokens:</span><br><span class="line">                byte_encoded_special_token = special_token.encode(<span class="string">"utf-8"</span>)</span><br><span class="line">                <span class="keyword">if</span> byte_encoded_special_token <span class="keyword">not</span> <span class="keyword">in</span> <span class="built_in">set</span>(vocab.values()):</span><br><span class="line">                    vocab[<span class="built_in">len</span>(vocab)] = byte_encoded_special_token</span><br><span class="line"></span><br><span class="line">        vocab = {</span><br><span class="line">            vocab_index: <span class="built_in">bytes</span>([c <span class="keyword">for</span> c <span class="keyword">in</span> vocab_item])</span><br><span class="line">            <span class="keyword">for</span> vocab_index, vocab_item <span class="keyword">in</span> vocab.items()</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        merges = [</span><br><span class="line">            (</span><br><span class="line">                <span class="built_in">bytes</span>([token <span class="keyword">for</span> token <span class="keyword">in</span> merge_token_1]),</span><br><span class="line">                <span class="built_in">bytes</span>([token <span class="keyword">for</span> token <span class="keyword">in</span> merge_token_2]),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> merge_token_1, merge_token_2 <span class="keyword">in</span> merges</span><br><span class="line">        ]</span><br><span class="line">        tokenizer = BPETokenizer(vocab, merges, special_tokens)</span><br><span class="line">        <span class="keyword">return</span> tokenizer</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">list</span>[<span class="built_in">int</span>]:</span><br><span class="line">        vocab_reversed = {v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="variable language_">self</span>.vocab.items()}</span><br><span class="line">        token_bytes_list = pre_tokenize(text, <span class="variable language_">self</span>.special_tokens) <span class="comment"># list[bytes]</span></span><br><span class="line"></span><br><span class="line">        byte_special_tokens = []</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.special_tokens:</span><br><span class="line">            byte_special_tokens = [special_token.encode(<span class="string">'utf-8'</span>) <span class="keyword">for</span> special_token <span class="keyword">in</span> <span class="variable language_">self</span>.special_tokens]</span><br><span class="line">            <span class="comment"># print("byte_special_tokens:", byte_special_tokens)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print((b'He', b'llo') in merges)</span></span><br><span class="line">        new_token_bytes_list = []</span><br><span class="line">        <span class="keyword">for</span> token_bytes <span class="keyword">in</span> token_bytes_list: <span class="comment"># bytes e.g. b'the'</span></span><br><span class="line">            <span class="comment"># print("initial token_bytes:", token_bytes)</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> token_bytes <span class="keyword">in</span> byte_special_tokens:</span><br><span class="line">                new_token_bytes_list.append(token_bytes)</span><br><span class="line">                <span class="comment"># print("special: ", token_bytes)</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 转换为list[bytes]</span></span><br><span class="line">            token_bytes = [<span class="built_in">bytes</span>([byte]) <span class="keyword">for</span> byte <span class="keyword">in</span> token_bytes]</span><br><span class="line">            <span class="comment"># print("token_bytes:", token_bytes)</span></span><br><span class="line">            <span class="comment"># print(token_bytes[0])</span></span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 必须按照词表的顺序</span></span><br><span class="line">            <span class="keyword">for</span> merge <span class="keyword">in</span> <span class="variable language_">self</span>.merges:</span><br><span class="line">                new_token_bytes = []</span><br><span class="line">                j = <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">                <span class="comment"># print(merge)</span></span><br><span class="line">                <span class="keyword">while</span> j &lt; <span class="built_in">len</span>(token_bytes):</span><br><span class="line">                    <span class="keyword">if</span> j &lt; <span class="built_in">len</span>(token_bytes) - <span class="number">1</span> <span class="keyword">and</span> (token_bytes[j], token_bytes[j+<span class="number">1</span>]) == merge:</span><br><span class="line">                        new_token_bytes.append(token_bytes[j] + token_bytes[j+<span class="number">1</span>])</span><br><span class="line">                        j += <span class="number">2</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        new_token_bytes.append(token_bytes[j])</span><br><span class="line">                        j += <span class="number">1</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># print(new_token_bytes)</span></span><br><span class="line">                token_bytes = new_token_bytes</span><br><span class="line">                <span class="comment"># print(len(new_token_bytes))</span></span><br><span class="line">            new_token_bytes_list.extend(new_token_bytes)</span><br><span class="line">            <span class="comment"># print("new_token_bytes_list after merged: ", new_token_bytes_list)</span></span><br><span class="line">        </span><br><span class="line">        new_token_ids_list = [vocab_reversed[i] <span class="keyword">for</span> i <span class="keyword">in</span> new_token_bytes_list]</span><br><span class="line">        <span class="comment"># print(new_token_ids_list)</span></span><br><span class="line">        <span class="keyword">return</span> new_token_ids_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_iterable</span>(<span class="params">self, iterable: Iterable[<span class="built_in">str</span>]</span>) -&gt; Iterator[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Given an iterable of</span></span><br><span class="line"><span class="string">        strings (e.g., a Python file handle), return a generator that lazily yields token IDs. This is</span></span><br><span class="line"><span class="string">        required for memory-eﬀicient tokenization of large files that we cannot directly load into memory</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> iterable:</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="variable language_">self</span>.encode(line):</span><br><span class="line">                <span class="keyword">yield</span> idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, ids: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># print("ids:", ids)</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="comment"># print(type(list(self.vocab.keys())[0])) # int</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> ids:</span><br><span class="line">            <span class="comment"># print(self.vocab[id].decode("utf-8"))</span></span><br><span class="line">            <span class="comment"># print(type(self.vocab.get(id, None)))</span></span><br><span class="line">            result.extend(<span class="variable language_">self</span>.vocab.get(<span class="built_in">id</span>, <span class="literal">None</span>))</span><br><span class="line">        <span class="comment"># print("decode result:", result)</span></span><br><span class="line">        result = <span class="built_in">bytes</span>(result).decode(<span class="string">"utf-8"</span>, errors=<span class="string">"replace"</span>)</span><br><span class="line">        <span class="comment"># print("decode result:", result)</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure>

    </div>

    
    
    

    <footer class="post-footer"><script src="//sdk.jinrishici.com/v2/browser/jinrishici.js"></script>
<script>
  jinrishici.load((result) => {
    let jrsc = document.getElementById('jrsc');
    const data = result.data;
    let author = data.origin.author;
    let title = '《' + data.origin.title + '》';
    let content = data.content.substr(0, data.content.length - 1);
    let dynasty = data.origin.dynasty.substr(0, data.origin.dynasty.length - 1);
    jrsc.innerText = content + ' @ ' + dynasty + '·' + author + title;
  });
</script>
<div style="text-align: center"><span id="jrsc" >正在加载今日诗词....</span></div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>原作者： </strong>濬哲
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.junzhe.top/posts/cs336-bpe-tokenizer.html" title="cs336-bpe 分词器">https://www.junzhe.top/posts/cs336-bpe-tokenizer.html</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
      </div>
  </div>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/56761.html" rel="prev" title="召回阶段负采样">
                  <i class="fa fa-angle-left"></i> 召回阶段负采样
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    
  
  <div class="comments giscus-container">
  </div>
  
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2024 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">濬哲</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">76k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:09</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>
<script class="next-config" data-name="giscus" type="application/json">{"enable":true,"repo":"DreamKongcheng/DreamKongcheng.github.io","repo_id":"R_kgDOM33FfA","category":"General","category_id":"DIC_kwDOM33FfM4CqJQz","mapping":"pathname","strict":0,"reactions_enabled":1,"emit_metadata":0,"theme":"light","lang":"zh-CN","crossorigin":"anonymous","input_position":"bottom","loading":"lazy"}</script>

<script>
document.addEventListener('page:loaded', () => {
  if (!CONFIG.page.comments) return;

  NexT.utils.loadComments('.giscus-container')
    .then(() => NexT.utils.getScript('https://giscus.app/client.js', {
      attributes: {
        async                   : true,
        crossOrigin             : 'anonymous',
        'data-repo'             : CONFIG.giscus.repo,
        'data-repo-id'          : CONFIG.giscus.repo_id,
        'data-category'         : CONFIG.giscus.category,
        'data-category-id'      : CONFIG.giscus.category_id,
        'data-mapping'          : CONFIG.giscus.mapping,
        'data-strict'           : CONFIG.giscus.strict,
        'data-reactions-enabled': CONFIG.giscus.reactions_enabled,
        'data-emit-metadata'    : CONFIG.giscus.emit_metadata,
        'data-theme'            : CONFIG.giscus.theme,
        'data-lang'             : CONFIG.giscus.lang,
        'data-input-position'   : CONFIG.giscus.input_position,
        'data-loading'          : CONFIG.giscus.loading
      },
      parentNode: document.querySelector('.giscus-container')
    }));
});
</script>

</body>
</html>
