<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.junzhe.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"æœç´¢...","empty":"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœï¼š${query}","hits_time":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰","hits":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœ"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="å‰è¨€ è¿™æ˜¯ cs336 è¯¾ç¨‹ assignment1 çš„ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼Œè¦æ±‚å®ç°ä¸€ä¸ª byte-level byte-pair encoding (BPE) åˆ†è¯å™¨ Byte-Level BPE æ˜¯ BPE çš„ä¸€ä¸ªå˜ä½“ï¼Œå…¶æœ€å…³é”®çš„åŒºåˆ«åœ¨äºï¼šå®ƒçš„æœ€åŸºç¡€å•å…ƒæ˜¯å­—èŠ‚ï¼ˆByteï¼‰ï¼Œè€Œä¸æ˜¯ Unicode å­—ç¬¦ã€‚ä¸€èˆ¬æˆ‘ä»¬è¯´åˆ°çš„ BPE æ˜¯åŸºäºå­—ç¬¦çš„ BPEï¼Œå¦‚æœæŒ‡æ˜äº†æ˜¯ Byte-Level BPEï¼Œé‚£ä¹ˆå®ƒçš„åŸº">
<meta property="og:type" content="article">
<meta property="og:title" content="cs336-bpe åˆ†è¯å™¨">
<meta property="og:url" content="https://www.junzhe.top/posts/cs336-bpe-tokenizer.html">
<meta property="og:site_name" content="æ¿¬å“²çš„åšå®¢">
<meta property="og:description" content="å‰è¨€ è¿™æ˜¯ cs336 è¯¾ç¨‹ assignment1 çš„ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼Œè¦æ±‚å®ç°ä¸€ä¸ª byte-level byte-pair encoding (BPE) åˆ†è¯å™¨ Byte-Level BPE æ˜¯ BPE çš„ä¸€ä¸ªå˜ä½“ï¼Œå…¶æœ€å…³é”®çš„åŒºåˆ«åœ¨äºï¼šå®ƒçš„æœ€åŸºç¡€å•å…ƒæ˜¯å­—èŠ‚ï¼ˆByteï¼‰ï¼Œè€Œä¸æ˜¯ Unicode å­—ç¬¦ã€‚ä¸€èˆ¬æˆ‘ä»¬è¯´åˆ°çš„ BPE æ˜¯åŸºäºå­—ç¬¦çš„ BPEï¼Œå¦‚æœæŒ‡æ˜äº†æ˜¯ Byte-Level BPEï¼Œé‚£ä¹ˆå®ƒçš„åŸº">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-09-10T03:19:56.000Z">
<meta property="article:modified_time" content="2025-09-10T08:00:20.189Z">
<meta property="article:author" content="æ¿¬å“²">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://www.junzhe.top/posts/cs336-bpe-tokenizer.html">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.junzhe.top/posts/cs336-bpe-tokenizer.html","path":"posts/cs336-bpe-tokenizer.html","title":"cs336-bpe åˆ†è¯å™¨"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>cs336-bpe åˆ†è¯å™¨ | æ¿¬å“²çš„åšå®¢</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="æ¿¬å“²çš„åšå®¢" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ " role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">æ¿¬å“²çš„åšå®¢</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">äººç”Ÿå¦‚æ£‹ï¼Œè½å­æ— æ‚”</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="æœç´¢" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>å…³äº</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>æ ‡ç­¾</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">å‰è¨€</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#unicode-standard"><span class="nav-number">2.</span> <span class="nav-text">Unicode Standard</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#unicode-encoding"><span class="nav-number">3.</span> <span class="nav-text">Unicode Encoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#subword-tokenization"><span class="nav-number">4.</span> <span class="nav-text">Subword Tokenization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bpe-tokenizer-training"><span class="nav-number">5.</span> <span class="nav-text">BPE Tokenizer Training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="nav-number">5.1.</span> <span class="nav-text">å®ç°ç»†èŠ‚</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pre-tokenization"><span class="nav-number">5.1.1.</span> <span class="nav-text">Pre-tokenization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bpe-merges"><span class="nav-number">5.1.2.</span> <span class="nav-text">BPE Merges</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bpe-tokenizer-encoding-and-decoding"><span class="nav-number">6.</span> <span class="nav-text">BPE Tokenizer Encoding and
Decoding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#encoding"><span class="nav-number">6.1.</span> <span class="nav-text">Encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decoding"><span class="nav-number">6.2.</span> <span class="nav-text">Decoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E7%9A%84bpe-tokenizer"><span class="nav-number">6.3.</span> <span class="nav-text">å®Œæ•´çš„ BPE Tokenizer</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="æ¿¬å“²"
      src="/images/head.jpg">
  <p class="site-author-name" itemprop="name">æ¿¬å“²</p>
  <div class="site-description" itemprop="description">æŠ€æœ¯æ²‰æ€ä¸äººé—´çƒŸç«</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/DreamKongcheng" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;DreamKongcheng" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3377965639@qq.com" title="E-Mail â†’ mailto:3377965639@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://dreamkongcheng.github.io/about/" title="Wechat â†’ https:&#x2F;&#x2F;dreamkongcheng.github.io&#x2F;about&#x2F;" rel="noopener me" target="_blank"><i class="fab fa-weixin fa-fw"></i>Wechat</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.junzhe.top/posts/cs336-bpe-tokenizer.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.jpg">
      <meta itemprop="name" content="æ¿¬å“²">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="æ¿¬å“²çš„åšå®¢">
      <meta itemprop="description" content="æŠ€æœ¯æ²‰æ€ä¸äººé—´çƒŸç«">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="cs336-bpe åˆ†è¯å™¨ | æ¿¬å“²çš„åšå®¢">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          cs336-bpe åˆ†è¯å™¨
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">å‘è¡¨äº</span>
      

      <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-10 11:19:56 / ä¿®æ”¹æ—¶é—´ï¼š16:00:20" itemprop="dateCreated datePublished" datetime="2025-09-10T11:19:56+08:00">2025-09-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">åˆ†ç±»äº</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/cs336/" itemprop="url" rel="index"><span itemprop="name">cs336</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="é˜…è¯»æ¬¡æ•°" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">é˜…è¯»æ¬¡æ•°ï¼š</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
      <span>11 åˆ†é’Ÿ</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="å‰è¨€">å‰è¨€</h2>
<p>è¿™æ˜¯ cs336 è¯¾ç¨‹ assignment1 çš„ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼Œè¦æ±‚å®ç°ä¸€ä¸ª byte-level
byte-pair encoding (BPE) åˆ†è¯å™¨</p>
<p>Byte-Level BPE
æ˜¯ BPE çš„ä¸€ä¸ªå˜ä½“ï¼Œå…¶æœ€å…³é”®çš„åŒºåˆ«åœ¨äºï¼šå®ƒçš„æœ€åŸºç¡€å•å…ƒæ˜¯å­—èŠ‚ï¼ˆByteï¼‰ï¼Œè€Œä¸æ˜¯ Unicode å­—ç¬¦ã€‚ä¸€èˆ¬æˆ‘ä»¬è¯´åˆ°çš„ BPE æ˜¯åŸºäºå­—ç¬¦çš„ BPEï¼Œå¦‚æœæŒ‡æ˜äº†æ˜¯ Byte-Level
BPEï¼Œé‚£ä¹ˆå®ƒçš„åŸºç¡€å•å…ƒå°±æ˜¯å­—èŠ‚ã€‚</p>
<p>åŸºäºå­—èŠ‚çš„ BPE çš„è¯è¡¨åªæœ‰ 256 ä¸ªå­—èŠ‚ï¼ˆ0-255ï¼‰ï¼Œå‡ ä¹ä¸ä¼šå‡ºç° oov çš„é—®é¢˜ã€‚è€ŒåŸºäºå­—ç¬¦çš„ BPE çš„è¯è¡¨å¤§å°åˆ™å–å†³äºè®­ç»ƒè¯­æ–™ä¸­å‡ºç°çš„å­—ç¬¦æ•°é‡ï¼Œé€šå¸¸ä¼šè¿œå¤§äº 256ï¼Œä¹Ÿä¼šå‡ºç° oov çš„é—®é¢˜ã€‚</p>
<span id="more"></span>
<h2 id="unicode-standard">Unicode Standard</h2>
<p>åœ¨äº†è§£ BPE ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆäº†è§£ Unicode Standardã€‚Unicode
Standard æ˜¯ä¸€ä¸ªå­—ç¬¦ç¼–ç æ ‡å‡†ï¼Œå®ƒä¸ºæ¯ä¸ªå­—ç¬¦åˆ†é…äº†ä¸€ä¸ªå”¯ä¸€çš„ä»£ç ç‚¹ï¼ˆcode
pointï¼‰ã€‚ä»£ç ç‚¹æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œé€šå¸¸ç”¨åå…­è¿›åˆ¶è¡¨ç¤ºï¼Œä¾‹å¦‚ U + 0041 è¡¨ç¤ºå­—ç¬¦â€™Aâ€™ã€‚</p>
<h2 id="unicode-encoding">Unicode Encoding</h2>
<p>Unicode ç¼–ç æ˜¯å°† Unicode ä»£ç ç‚¹è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—çš„è¿‡ç¨‹ã€‚å¸¸è§çš„ Unicode ç¼–ç æœ‰ UTF-8ã€UTF-16 å’Œ UTF-32ã€‚UTF-8 æ˜¯æœ€å¸¸ç”¨çš„ Unicode ç¼–ç ï¼Œå®ƒä½¿ç”¨ 1 åˆ° 4 ä¸ªå­—èŠ‚è¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ã€‚UTF-8 çš„ä¼˜ç‚¹æ˜¯å…¼å®¹ ASCII ç¼–ç ï¼Œå¹¶ä¸”å¯¹äºå¸¸ç”¨å­—ç¬¦ä½¿ç”¨è¾ƒå°‘çš„å­—èŠ‚è¡¨ç¤ºã€‚</p>
<p>cs336 ä½œä¸šä¸­çš„ä¸€æ®µä»‹ç»ï¼š &gt; While the Unicode standard defines a
mapping from characters to code points (integers), itâ€™s impractical to
train tokenizers directly on Unicode codepoints, since the vocabulary
would be prohibitively large (around 150K items) and sparse (since many
characters are quite rare). Instead, weâ€™ll use a Unicode encoding, which
converts a Unicode character into a sequence of bytes</p>
<p>To encode a Unicode string into UTF-8, we can use the
<code>encode()</code> function in Python </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_string = <span class="string">"hello! ã“ã‚“ã«ã¡ã¯!"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>utf8_encoded = test_string.encode(<span class="string">"utf-8"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(utf8_encoded)</span><br><span class="line"><span class="string">b'hello! \xe3\x81\x93\xe3\x82\x93\xe3\x81\xab\xe3\x81\xa1\xe3\x81\xaf!'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">type</span>(utf8_encoded))</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">'bytes'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Get the byte values for the encoded string (integers from 0 to 255).</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(utf8_encoded)</span><br><span class="line">[<span class="number">104</span>, <span class="number">101</span>, <span class="number">108</span>, <span class="number">108</span>, <span class="number">111</span>, <span class="number">33</span>, <span class="number">32</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">147</span>, <span class="number">227</span>, <span class="number">130</span>, <span class="number">147</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">171</span>, <span class="number">227</span>, <span class="number">129</span>,</span><br><span class="line"><span class="number">161</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">175</span>, <span class="number">33</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># One byte does not necessarily correspond to one Unicode character!</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">len</span>(test_string))</span><br><span class="line"><span class="number">13</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">len</span>(utf8_encoded))</span><br><span class="line"><span class="number">23</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(utf8_encoded.decode(<span class="string">"utf-8"</span>))</span><br><span class="line">hello! ã“ã‚“ã«ã¡ã¯!</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>By converting our Unicode codepoints into a sequence of bytes (e.g.,
via the UTF-8 encoding), we are essentially taking a sequence of
codepoints (integers in the range 0 to 154,997) and transforming it into
a sequence of byte values (integers in the range 0 to 255).</p>
<p>å…·ä½“çš„ç¼–ç è§„åˆ™ä¸è¿›è¡Œå±•å¼€äº†</p>
<h2 id="subword-tokenization">Subword Tokenization</h2>
<p>While byte-level tokenization can alleviate the out-of-vocabulary
issues faced by word-level tokenizers, tokenizing text into bytes
results in extremely long input sequences. è™½ç„¶ byte-level
tokenization å¯ä»¥è§£å†³ word-level
tokenizers å¯¼è‡´çš„ oov çš„é—®é¢˜ï¼Œä½†æ˜¯è¿™æ ·ä¼šå¯¼è‡´è¾“å…¥åºåˆ—éå¸¸é•¿ã€‚</p>
<p>Subword tokenization is a midpoint between word-level tokenizers and
byte-level tokenizers.</p>
<p>æˆ‘ä»¬ä¼šæŠŠé‚£äº›ç»å¸¸å‡ºç°åœ¨ä¸€èµ·çš„å­—èŠ‚å¯¹ï¼ˆbyte
pairï¼‰åˆå¹¶æˆä¸€ä¸ªæ–°çš„å­è¯ï¼ˆsubwordï¼‰ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨ä¿æŒè¾ƒå°è¯è¡¨çš„åŒæ—¶ï¼Œå‡å°‘è¾“å…¥åºåˆ—çš„é•¿åº¦ã€‚</p>
<p>For example, if the byte sequence bâ€™theâ€™ often occurs in our raw text
training data, assigning it an entry in the vocabulary would reduce this
3-token sequence to a single token.</p>
<h2 id="bpe-tokenizer-training">BPE Tokenizer Training</h2>
<p>BPE åˆ†è¯å™¨çš„è®­ç»ƒä¸»è¦åˆ†ä¸º 3 æ­¥</p>
<ol type="1">
<li><strong>Vocabulary initialization</strong>:
è¯è¡¨æ˜¯ä¸€ä¸ªä» bytestring åˆ° int çš„ä¸€ä¸€æ˜ å°„ã€‚ç”±äºæˆ‘ä»¬é‡‡ç”¨çš„æ˜¯ byte-level
BPEï¼Œæ‰€ä»¥è¯è¡¨çš„åˆå§‹å¤§å°å°±æ˜¯ 256</li>
<li><strong>Pre-tokenization</strong>:
<ul>
<li>ç†è®ºä¸Šï¼Œæœ‰äº†åˆå§‹çš„è¯è¡¨ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥ç›´æ¥éå†æ‰€æœ‰çš„å­—èŠ‚å¯¹ï¼Œç„¶åæŠŠå‡ºç°é¢‘ç‡æœ€é«˜çš„å­—èŠ‚å¯¹åˆå¹¶æˆä¸€ä¸ªæ–°çš„å­è¯ï¼ŒåŠ å…¥åˆ°è¯è¡¨ä¸­ã€‚ä½†æ˜¯è¿™æ ·åšæ•ˆç‡éå¸¸ä½ï¼Œå› ä¸ºæ¯ä¸€æ¬¡æˆ‘ä»¬éƒ½è¦éå†æ•´ä¸€ä¸ªè¯­æ–™åº“ã€‚å¦å¤–ç›´æ¥åˆå¹¶å­—èŠ‚å¯èƒ½ä¼šäº§ç”Ÿè·¨å•è¯è¾¹ç•Œçš„ tokenï¼Œæˆ–è€…åˆ›å»ºå‡ºä»…åœ¨æ ‡ç‚¹ç¬¦å·ä¸Šæœ‰å·®å¼‚çš„å†—ä½™ tokenï¼ˆå¦‚ dog å’Œ dog! è¢«è§†ä¸ºå®Œå…¨ä¸åŒçš„ tokenï¼‰ï¼Œè¿™æµªè´¹äº†è¯æ±‡è¡¨ç©ºé—´ä¸”ä¸ç¬¦åˆè¯­è¨€è§„å¾‹ã€‚</li>
<li>åŠ ä¸Šäº†é¢„åˆ†è¯ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å…ˆæŠŠè¯­æ–™åº“åˆ†æˆä¸€ä¸ªä¸ªå•è¯ï¼Œç„¶ååœ¨æ¯ä¸ªå•è¯å†…éƒ¨è¿›è¡Œ BPE åˆå¹¶ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ä¸éœ€è¦å†å»åŸå§‹çš„å­—èŠ‚æµä¸­ç»Ÿè®¡é¢‘ç‡ï¼Œè€Œæ˜¯å¯ä»¥ç›´æ¥ç»Ÿè®¡è¿™äº› â€œé¢„ tokenâ€ ä¸­å„ä¸ªå­—èŠ‚å¯¹çš„é¢‘ç‡ã€‚å¦å¤–ï¼Œé¢„åˆ†è¯ä¹Ÿèƒ½é¿å…è·¨å•è¯è¾¹ç•Œçš„ tokenï¼ˆä¸€èˆ¬éƒ½ä¸è·¨å•è¯è¾¹ç•Œåˆå¹¶ï¼‰ã€‚</li>
<li>ä¾‹å­ï¼šå¦‚æœå•è¯â€ textâ€ åœ¨è¯­æ–™ä¸­å‡ºç°äº† 10 æ¬¡ï¼Œé‚£ä¹ˆå½“æˆ‘ä»¬ç»Ÿè®¡å­—èŠ‚â€™tâ€™å’Œâ€™eâ€™åœ¨â€ textâ€ å†…éƒ¨ç›¸é‚»çš„é¢‘ç‡æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä¸ºè¿™ä¸ªè¯å¯¹ (t,
e) çš„é¢‘ç‡åŠ ä¸Š 10ï¼Œè€Œä¸æ˜¯åœ¨åŸå§‹è¯­æ–™ä¸­æ‰«æ 10 æ¬¡ã€‚è¿™å¤§å¤§æå‡äº†ç»Ÿè®¡æ•ˆç‡ã€‚</li>
</ul></li>
<li><strong>Compute BPE merges</strong>:
ç°åœ¨å·²ç»æŠŠè¾“å…¥å˜æˆäº† pre-tokns å¹¶ä¸”æ¯ä¸€ä¸ª pre-token éƒ½æ˜¯ä¸€ä¸ª utf-8 ç¼–ç çš„ bytestringã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥ç»Ÿè®¡è¿™äº› pre-token ä¸­å„ä¸ªå­—èŠ‚å¯¹çš„é¢‘ç‡ï¼Œç„¶åæŠŠå‡ºç°é¢‘ç‡æœ€é«˜çš„å­—èŠ‚å¯¹åˆå¹¶æˆä¸€ä¸ªæ–°çš„å­è¯ï¼ŒåŠ å…¥åˆ°è¯è¡¨ä¸­ã€‚å¹¶ä¸”æ¥ä¸‹æ¥è¦æŠŠæ‰€æœ‰åŒ…å«è¿™ä¸ªå­—èŠ‚å¯¹çš„ pre-token éƒ½è¿›è¡Œæ›´æ–°ï¼ˆåˆå¹¶è¿™ä¸ªå­—èŠ‚å¯¹ï¼‰ã€‚é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°è¯è¡¨è¾¾åˆ°æŒ‡å®šçš„å¤§å°ã€‚ä½œä¸šä¸­è¦æ±‚å¦‚æœæœ‰å¤šä¸ªå­—èŠ‚å¯¹é¢‘ç‡ç›¸åŒï¼Œé‚£ä¹ˆå°±é€‰æ‹©å­—å…¸åºæœ€å¤§çš„é‚£ä¸ªå­—èŠ‚å¯¹è¿›è¡Œåˆå¹¶ã€‚</li>
</ol>
<p>å¦å¤–éœ€è¦æ³¨æ„ special tokens çš„å¤„ç†ã€‚special
tokens æ˜¯ä¸€äº›ç‰¹æ®Šçš„ tokenï¼Œæ¯”å¦‚ [PAD]ã€[UNK]ã€[CLS]ã€[SEP]ã€&lt;|endoftext|&gt; ç­‰ã€‚è¿™äº› token åœ¨è®­ç»ƒ BPE åˆ†è¯å™¨æ—¶éœ€è¦è¢«åŠ å…¥åˆ°è¯è¡¨ä¸­ï¼Œå¹¶ä¸”ä¸èƒ½è¢«ä¿®æ”¹ã€‚</p>
<h3 id="å®ç°ç»†èŠ‚">å®ç°ç»†èŠ‚</h3>
<h4 id="pre-tokenization">Pre-tokenization</h4>
<p>é¦–å…ˆæ˜¯é¢„åˆ†è¯éƒ¨åˆ†ï¼Œè¿™ä¸€éƒ¨åˆ†æ˜¯å¯ä»¥è¿›è¡Œå¹¶è¡Œçš„ã€‚å®éªŒè¦æ±‚ä¸­è¯´ pre-tokenize æ˜¯ä¸»è¦çš„æ€§èƒ½ç“¶é¢ˆï¼Œè€Œè¿™ä¸€éƒ¨åˆ†ç“¶é¢ˆæ˜¯å¯ä»¥é€šè¿‡å¹¶è¡Œè§£å†³ï¼Œæ€è·¯å°±æ˜¯æŠŠè¯­æ–™åº“åˆ†æˆå¤šä¸ª chunkï¼Œç„¶åæ¯ä¸ª chunk äº¤ç»™ä¸€ä¸ªçº¿ç¨‹è¿›è¡Œé¢„åˆ†è¯ï¼Œæœ€åæŠŠæ‰€æœ‰çº¿ç¨‹çš„ç»“æœåˆå¹¶èµ·æ¥ã€‚</p>
<p>ç„¶åæ³¨æ„è¿™ä¸€éƒ¨åˆ†æ˜¯ä¸èƒ½åŒ…å« special tokens çš„ã€‚</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pre_tokenize</span>(<span class="params">text: <span class="built_in">str</span>, special_tokens: <span class="built_in">list</span>[<span class="built_in">str</span>] | <span class="literal">None</span> = <span class="literal">None</span>, preserve_special_tokens: <span class="built_in">bool</span> = <span class="literal">True</span></span>) -&gt; <span class="built_in">list</span>[<span class="built_in">bytes</span>]:</span><br><span class="line">    docs = split_by_special_tokens(text, special_tokens)</span><br><span class="line">    <span class="comment"># print("\ndocs: ", docs) # ['HÃ©llÃ² hÃ´w ', '&lt;|endoftext|&gt;', '', '&lt;|endoftext|&gt;', ' are Ã¼? ğŸ™ƒ', '&lt;|endoftext|&gt;', '']</span></span><br><span class="line">    PAT = PAT = <span class="string">r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""</span></span><br><span class="line">    token_bytes_list = []</span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> docs:</span><br><span class="line">        <span class="keyword">if</span> special_tokens <span class="keyword">and</span> doc <span class="keyword">in</span> special_tokens:</span><br><span class="line">            <span class="keyword">if</span> preserve_special_tokens:</span><br><span class="line">                special_token_bytes = doc.encode(<span class="string">"utf-8"</span>)</span><br><span class="line">                token_bytes_list.extend([special_token_bytes])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tokens = re.findall(PAT, doc) <span class="comment"># ['the', ' cat', ' ate']</span></span><br><span class="line">            token_bytes = [t.encode(<span class="string">"utf-8"</span>) <span class="keyword">for</span> t <span class="keyword">in</span> tokens] <span class="comment"># [b'the', b' cat', b' ate'].</span></span><br><span class="line">            token_bytes_list.extend(token_bytes)</span><br><span class="line">    <span class="comment"># print("token_bytes_list:", token_bytes_list)</span></span><br><span class="line">    <span class="keyword">return</span> token_bytes_list</span><br></pre></td></tr></tbody></table></figure>
<h4 id="bpe-merges">BPE Merges</h4>
<p>è¿™ä¸€éƒ¨åˆ†æ˜¯æ•´ä¸ª BPE è®­ç»ƒçš„æ ¸å¿ƒéƒ¨åˆ†ï¼ŒåŸºæœ¬çš„ç®—æ³•ä¹Ÿå¾ˆç›´è§‚ã€‚é¦–å…ˆæ„é€ ä¸€ä¸ª word_cnt å­—å…¸ï¼Œkey æ˜¯ pre-token çš„ bytestringï¼Œvalue æ˜¯è¿™ä¸ª pre-token åœ¨è¯­æ–™ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚ç„¶åç»Ÿè®¡æ‰€æœ‰ pre-token ä¸­ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡ï¼Œé€‰æ‹©é¢‘ç‡æœ€é«˜çš„å­—èŠ‚å¯¹è¿›è¡Œåˆå¹¶ï¼Œæ›´æ–° word_cntï¼Œæ›´æ–° merges åˆ—è¡¨ï¼Œæ›´æ–° vocabã€‚é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°è¯è¡¨è¾¾åˆ°æŒ‡å®šçš„å¤§å°ã€‚</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">merges = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">    pair_cnt = count_pair(word_cnt)</span><br><span class="line">    max_pair = MAX(pair_cnt)</span><br><span class="line">    merges.append(max_pair)</span><br><span class="line">    vocab[base_vocab_size + i] = max_pair  <span class="comment"># ç›´æ¥èµ‹å€¼</span></span><br><span class="line"></span><br><span class="line">    new_word_cnt = {}</span><br><span class="line">    <span class="keyword">for</span> word_bytes, cnt <span class="keyword">in</span> word_cnt.items():</span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        new_word_bytes = []</span><br><span class="line">        <span class="keyword">while</span> j &lt; <span class="built_in">len</span>(word_bytes):</span><br><span class="line">            <span class="keyword">if</span> j &lt; <span class="built_in">len</span>(word_bytes) - <span class="number">1</span> <span class="keyword">and</span> (word_bytes[j], word_bytes[j+<span class="number">1</span>]) == max_pair:</span><br><span class="line">                new_word_bytes.append(max_pair[<span class="number">0</span>] + max_pair[<span class="number">1</span>])</span><br><span class="line">                j += <span class="number">2</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_word_bytes.append(word_bytes[j])</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">        new_word_bytes = <span class="built_in">tuple</span>(new_word_bytes)</span><br><span class="line">        new_word_cnt[new_word_bytes] = new_word_cnt.get(new_word_bytes, <span class="number">0</span>) + cnt</span><br><span class="line">    word_cnt = new_word_cnt  <span class="comment"># æ›´æ–°word_cnt</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> vocab, merges</span><br></pre></td></tr></tbody></table></figure>
<p>å½“ç„¶ï¼Œä¸Šé¢çš„ä»£ç æ˜¯æ²¡æ³•è¿‡æµ‹è¯•çš„ï¼Œå› ä¸ºæ—¶é—´å¤æ‚åº¦å¾ˆé«˜ï¼Œæœ‰å¦‚ä¸‹å‡ ä¸ªåœ°æ–¹å¯ä»¥è¿›è¡Œä¼˜åŒ–
1. æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„å­—èŠ‚å¯¹æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å †ï¼ˆheapï¼‰æ¥ä¼˜åŒ– 2.
æ›´æ–° word_cnt æ—¶ï¼Œå¯ä»¥åªæ›´æ–°åŒ…å« max_pair çš„ pre-tokenï¼Œè€Œä¸æ˜¯æ‰€æœ‰çš„ pre-tokenï¼Œè¿™é‡Œå¯ä»¥ä½¿ç”¨å€’æ’ç´¢å¼•æ¥è®°å½•åŒ…å«æ¯ä¸ªå­—èŠ‚å¯¹çš„ pre-token åˆ—è¡¨</p>
<p>æ€»ä¹‹ï¼Œä¼˜åŒ–çš„æ€è·¯å°±æ˜¯ä½¿ç”¨æ›´åŠ é«˜æ•ˆçš„æ•°æ®ç»“æ„ä»¥åŠå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ï¼Œå®Œæ•´çš„ä¼˜åŒ–åä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bpe</span>(<span class="params"></span></span><br><span class="line"><span class="params">    input_path: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    vocab_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    special_tokens: <span class="built_in">list</span>[<span class="built_in">str</span>]</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="built_in">tuple</span>[<span class="built_in">dict</span>[<span class="built_in">int</span>, <span class="built_in">bytes</span>], <span class="built_in">list</span>[<span class="built_in">tuple</span>[<span class="built_in">bytes</span>, <span class="built_in">bytes</span>]]]:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(input_path, <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        num_processes = <span class="number">8</span></span><br><span class="line">        boundaries = find_chunk_boundaries(f, num_processes, <span class="string">b"&lt;|endoftext|&gt;"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The following is a serial implementation, but you can parallelize this</span></span><br><span class="line">        <span class="comment"># by sending each start/end pair to a set of processes.</span></span><br><span class="line">        chunks = []</span><br><span class="line">        <span class="keyword">for</span> start, end <span class="keyword">in</span> <span class="built_in">zip</span>(boundaries[:-<span class="number">1</span>], boundaries[<span class="number">1</span>:]):</span><br><span class="line">            f.seek(start)</span><br><span class="line">            chunk = f.read(end - start).decode(<span class="string">"utf-8"</span>, errors=<span class="string">"ignore"</span>)</span><br><span class="line">            <span class="comment"># Run pre-tokenization on your chunk and store the counts for each pre-token</span></span><br><span class="line">            chunks.append(chunk)</span><br><span class="line"></span><br><span class="line">        args = [(chunk, special_tokens) <span class="keyword">for</span> chunk <span class="keyword">in</span> chunks]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> multiprocessing.Pool(processes=num_processes) <span class="keyword">as</span> pool:</span><br><span class="line">            results = pool.starmap(process_chunk, args)</span><br><span class="line">            </span><br><span class="line">        word_cnt = Counter()</span><br><span class="line">        <span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">            word_cnt.update(result)</span><br><span class="line">    </span><br><span class="line">        vocab = get_basic_vocab(special_tokens)</span><br><span class="line">        base_vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line">        num_merges = vocab_size - base_vocab_size</span><br><span class="line"></span><br><span class="line">        pair_cnt, pair2word_bytes = count_pair(word_cnt)</span><br><span class="line"></span><br><span class="line">        merges = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> i  % <span class="number">100</span> == <span class="number">0</span>:  <span class="comment"># æ¯100è½®é‡å»ºä¸€æ¬¡å †ï¼Œé¿å…å †å˜å¾—å¤ªå¤§</span></span><br><span class="line">                heap = MaxHeap()</span><br><span class="line">                <span class="keyword">for</span> pair, cnt <span class="keyword">in</span> pair_cnt.items():</span><br><span class="line">                    heap.push((cnt, pair))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># æ‡’æƒ°åˆ é™¤</span></span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                cnt, pair = heap.pop()</span><br><span class="line">                <span class="comment"># print(cnt, pair)</span></span><br><span class="line">                <span class="keyword">if</span> pair <span class="keyword">in</span> pair_cnt <span class="keyword">and</span> pair_cnt[pair] == cnt:</span><br><span class="line">                    max_pair = pair</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="comment"># å¦åˆ™ä¸¢å¼ƒï¼Œç»§ç»­å¼¹å‡º</span></span><br><span class="line">            </span><br><span class="line">            merges.append(max_pair)</span><br><span class="line">            vocab[base_vocab_size + i] = max_pair[<span class="number">0</span>] + max_pair[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># æ›´æ–°word_cnt</span></span><br><span class="line">            <span class="comment"># åªéœ€è¦æ”¹åŠ¨é‚£äº›max_pairåˆå¹¶å½±å“åˆ°çš„</span></span><br><span class="line">            affected_word_bytes = pair2word_bytes[max_pair] <span class="comment"># set</span></span><br><span class="line">            pair2word_bytes.pop(max_pair)</span><br><span class="line">            affected_pairs = <span class="built_in">set</span>()</span><br><span class="line">            <span class="keyword">for</span> word_bytes <span class="keyword">in</span> affected_word_bytes:</span><br><span class="line">                cnt = word_cnt[word_bytes]</span><br><span class="line">                word_cnt.pop(word_bytes)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(word_bytes[:-<span class="number">1</span>], word_bytes[<span class="number">1</span>:]):</span><br><span class="line">                    pair_cnt[pair] -= cnt</span><br><span class="line">                    <span class="keyword">if</span> pair_cnt[pair] == <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">del</span> pair_cnt[pair]</span><br><span class="line">                    pair2word_bytes[pair].discard(word_bytes)</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> pair2word_bytes[pair]:</span><br><span class="line">                        <span class="keyword">del</span> pair2word_bytes[pair]</span><br><span class="line"></span><br><span class="line">                    affected_pairs.add(pair)  <span class="comment"># å› ä¸ºæœ‰äº›pairåŠæ—¶å˜å°‘äº†ï¼Œå¤šè½®ä»¥åå¯èƒ½ä¹Ÿæ˜¯æœ€å¤§çš„ï¼Œæ‰€ä»¥è¿™é‡Œå¿…é¡»è¦è¿½è¸ª</span></span><br><span class="line"></span><br><span class="line">                j = <span class="number">0</span></span><br><span class="line">                new_word_bytes = []</span><br><span class="line">                <span class="keyword">while</span> j &lt; <span class="built_in">len</span>(word_bytes):</span><br><span class="line">                    <span class="keyword">if</span> j &lt; <span class="built_in">len</span>(word_bytes) - <span class="number">1</span> <span class="keyword">and</span> (word_bytes[j], word_bytes[j+<span class="number">1</span>]) == max_pair: <span class="comment"># é‡åˆ°è¦åˆå¹¶çš„æƒ…å†µ</span></span><br><span class="line">                        new_word_bytes.append(max_pair[<span class="number">0</span>] + max_pair[<span class="number">1</span>])</span><br><span class="line">                        j += <span class="number">2</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        new_word_bytes.append(word_bytes[j])</span><br><span class="line">                        j += <span class="number">1</span></span><br><span class="line">                new_word_bytes = <span class="built_in">tuple</span>(new_word_bytes)</span><br><span class="line">                word_cnt[new_word_bytes] += cnt</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> i, pair <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(new_word_bytes[:-<span class="number">1</span>], new_word_bytes[<span class="number">1</span>:])):</span><br><span class="line">                    pair_cnt[pair] += cnt</span><br><span class="line">                    pair2word_bytes[pair].add(new_word_bytes)</span><br><span class="line">                    affected_pairs.add(pair)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> pair <span class="keyword">in</span> affected_pairs:</span><br><span class="line">                <span class="keyword">if</span> pair <span class="keyword">in</span> pair_cnt:</span><br><span class="line">                    heap.push((pair_cnt[pair], pair))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab, merges</span><br></pre></td></tr></tbody></table></figure>
<p>è¿™æ®µä»£ç è·‘å®Œç¬¬ä¸€ä¸ªæµ‹è¯•ç‚¹åªéœ€è¦ 0.3sï¼Œè¿œè¶…è¿‡ 1.5s çš„è¦æ±‚</p>
<h2 id="bpe-tokenizer-encoding-and-decoding">BPE Tokenizer Encoding and
Decoding</h2>
<p>BPE åˆ†è¯å™¨çš„ç¼–ç å’Œè§£ç ç›¸å¯¹ç®€å•ï¼Œä¸»è¦æ˜¯æ ¹æ®è®­ç»ƒå¥½çš„è¯è¡¨å’Œ merges è¿›è¡Œæ“ä½œã€‚</p>
<h3 id="encoding">Encoding</h3>
<ol type="1">
<li><strong>Pre-tokenization</strong>:
è¿™ä¸€éƒ¨åˆ†å’Œè®­ç»ƒæ—¶çš„é¢„åˆ†è¯æ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯æŠŠè¾“å…¥æ–‡æœ¬åˆ†æˆä¸€ä¸ªä¸ª pre-tokenã€‚</li>
<li><strong>Apply the merges</strong>:
å¯¹æ¯ä¸ª pre-tokenï¼Œå…ˆæ‹†æˆä¸€ä¸ªä¸ªçš„å­—èŠ‚ï¼Œç„¶å<strong>æ ¹æ® merges åˆ—è¡¨çš„é¡ºåº</strong>è¿›è¡Œåˆå¹¶ï¼Œç›´åˆ°ä¸èƒ½å†åˆå¹¶ä¸ºæ­¢ã€‚æœ€åæŠŠæ‰€æœ‰çš„ token çš„ id æ‹¼æ¥èµ·æ¥ï¼Œä½œä¸ºæœ€ç»ˆçš„ç¼–ç ç»“æœã€‚
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å¿…é¡»æŒ‰ç…§è¯è¡¨çš„é¡ºåº</span></span><br><span class="line"><span class="keyword">for</span> merge <span class="keyword">in</span> <span class="variable language_">self</span>.merges:</span><br><span class="line">    new_token_bytes = []</span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(merge)</span></span><br><span class="line">    <span class="keyword">while</span> j &lt; <span class="built_in">len</span>(token_bytes):</span><br><span class="line">        <span class="keyword">if</span> j &lt; <span class="built_in">len</span>(token_bytes) - <span class="number">1</span> <span class="keyword">and</span> (token_bytes[j], token_bytes[j+<span class="number">1</span>]) == merge:</span><br><span class="line">            new_token_bytes.append(token_bytes[j] + token_bytes[j+<span class="number">1</span>])</span><br><span class="line">            j += <span class="number">2</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_token_bytes.append(token_bytes[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># print(new_token_bytes)</span></span><br><span class="line">    token_bytes = new_token_bytes</span><br></pre></td></tr></tbody></table></figure>
ç”±æ­¤å¯è§ï¼Œæ¯ä¸€ä¸ª token éƒ½è¦æŠŠ merge è¡¨ä»å¤´åˆ°å°¾éå†ä¸€éï¼Œæ—¶é—´å¤æ‚åº¦æ˜¯ O (n * m)</li>
</ol>
<p>å¦å¤–éœ€è¦æ³¨æ„ï¼Œspecial tokens éœ€è¦è¢«ä¿ç•™</p>
<h3 id="decoding">Decoding</h3>
<p>è§£ç ç›¸å¯¹ç®€å•ï¼Œå°±æ˜¯æŠŠ token
id è½¬æ¢æˆ bytesï¼ˆé€šè¿‡ vocabï¼‰ï¼Œç„¶åæ‹¼æ¥æˆä¸€ä¸ª bytestring</p>
<h3 id="å®Œæ•´çš„bpe-tokenizer">å®Œæ•´çš„ BPE Tokenizer</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BPETokenizer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, </span></span><br><span class="line"><span class="params">        vocab:  <span class="built_in">dict</span>[<span class="built_in">int</span>, <span class="built_in">bytes</span>],</span></span><br><span class="line"><span class="params">        merges:  <span class="built_in">list</span>[<span class="built_in">tuple</span>[<span class="built_in">bytes</span>, <span class="built_in">bytes</span>]],</span></span><br><span class="line"><span class="params">        special_tokens: <span class="built_in">list</span>[<span class="built_in">str</span>] | <span class="literal">None</span> = <span class="literal">None</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="variable language_">self</span>.vocab = vocab</span><br><span class="line">        <span class="variable language_">self</span>.merges = merges</span><br><span class="line">        <span class="variable language_">self</span>.special_tokens = special_tokens</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_files</span>(<span class="params"></span></span><br><span class="line"><span class="params">        cls, </span></span><br><span class="line"><span class="params">        vocab_filepath: <span class="built_in">str</span>, </span></span><br><span class="line"><span class="params">        merges_filepath: <span class="built_in">str</span>, </span></span><br><span class="line"><span class="params">        special_tokens: <span class="built_in">list</span>[<span class="built_in">str</span>] | <span class="literal">None</span> = <span class="literal">None</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(vocab_filepath) <span class="keyword">as</span> vocab_f:</span><br><span class="line">            vocab = json.load(vocab_f)</span><br><span class="line">        merges = []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(merges_filepath) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                cleaned_line = line.rstrip()</span><br><span class="line">                <span class="keyword">if</span> cleaned_line <span class="keyword">and</span> <span class="built_in">len</span>(cleaned_line.split(<span class="string">" "</span>)) == <span class="number">2</span>:</span><br><span class="line">                    merges.append(<span class="built_in">tuple</span>(cleaned_line.split(<span class="string">" "</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> special_tokens:</span><br><span class="line">            <span class="keyword">for</span> special_token <span class="keyword">in</span> special_tokens:</span><br><span class="line">                byte_encoded_special_token = special_token.encode(<span class="string">"utf-8"</span>)</span><br><span class="line">                <span class="keyword">if</span> byte_encoded_special_token <span class="keyword">not</span> <span class="keyword">in</span> <span class="built_in">set</span>(vocab.values()):</span><br><span class="line">                    vocab[<span class="built_in">len</span>(vocab)] = byte_encoded_special_token</span><br><span class="line"></span><br><span class="line">        vocab = {</span><br><span class="line">            vocab_index: <span class="built_in">bytes</span>([c <span class="keyword">for</span> c <span class="keyword">in</span> vocab_item])</span><br><span class="line">            <span class="keyword">for</span> vocab_index, vocab_item <span class="keyword">in</span> vocab.items()</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        merges = [</span><br><span class="line">            (</span><br><span class="line">                <span class="built_in">bytes</span>([token <span class="keyword">for</span> token <span class="keyword">in</span> merge_token_1]),</span><br><span class="line">                <span class="built_in">bytes</span>([token <span class="keyword">for</span> token <span class="keyword">in</span> merge_token_2]),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> merge_token_1, merge_token_2 <span class="keyword">in</span> merges</span><br><span class="line">        ]</span><br><span class="line">        tokenizer = BPETokenizer(vocab, merges, special_tokens)</span><br><span class="line">        <span class="keyword">return</span> tokenizer</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">list</span>[<span class="built_in">int</span>]:</span><br><span class="line">        vocab_reversed = {v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="variable language_">self</span>.vocab.items()}</span><br><span class="line">        token_bytes_list = pre_tokenize(text, <span class="variable language_">self</span>.special_tokens) <span class="comment"># list[bytes]</span></span><br><span class="line"></span><br><span class="line">        byte_special_tokens = []</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.special_tokens:</span><br><span class="line">            byte_special_tokens = [special_token.encode(<span class="string">'utf-8'</span>) <span class="keyword">for</span> special_token <span class="keyword">in</span> <span class="variable language_">self</span>.special_tokens]</span><br><span class="line">            <span class="comment"># print("byte_special_tokens:", byte_special_tokens)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print((b'He', b'llo') in merges)</span></span><br><span class="line">        new_token_bytes_list = []</span><br><span class="line">        <span class="keyword">for</span> token_bytes <span class="keyword">in</span> token_bytes_list: <span class="comment"># bytes e.g. b'the'</span></span><br><span class="line">            <span class="comment"># print("initial token_bytes:", token_bytes)</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> token_bytes <span class="keyword">in</span> byte_special_tokens:</span><br><span class="line">                new_token_bytes_list.append(token_bytes)</span><br><span class="line">                <span class="comment"># print("special: ", token_bytes)</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># è½¬æ¢ä¸ºlist[bytes]</span></span><br><span class="line">            token_bytes = [<span class="built_in">bytes</span>([byte]) <span class="keyword">for</span> byte <span class="keyword">in</span> token_bytes]</span><br><span class="line">            <span class="comment"># print("token_bytes:", token_bytes)</span></span><br><span class="line">            <span class="comment"># print(token_bytes[0])</span></span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># å¿…é¡»æŒ‰ç…§è¯è¡¨çš„é¡ºåº</span></span><br><span class="line">            <span class="keyword">for</span> merge <span class="keyword">in</span> <span class="variable language_">self</span>.merges:</span><br><span class="line">                new_token_bytes = []</span><br><span class="line">                j = <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">                <span class="comment"># print(merge)</span></span><br><span class="line">                <span class="keyword">while</span> j &lt; <span class="built_in">len</span>(token_bytes):</span><br><span class="line">                    <span class="keyword">if</span> j &lt; <span class="built_in">len</span>(token_bytes) - <span class="number">1</span> <span class="keyword">and</span> (token_bytes[j], token_bytes[j+<span class="number">1</span>]) == merge:</span><br><span class="line">                        new_token_bytes.append(token_bytes[j] + token_bytes[j+<span class="number">1</span>])</span><br><span class="line">                        j += <span class="number">2</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        new_token_bytes.append(token_bytes[j])</span><br><span class="line">                        j += <span class="number">1</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># print(new_token_bytes)</span></span><br><span class="line">                token_bytes = new_token_bytes</span><br><span class="line">                <span class="comment"># print(len(new_token_bytes))</span></span><br><span class="line">            new_token_bytes_list.extend(new_token_bytes)</span><br><span class="line">            <span class="comment"># print("new_token_bytes_list after merged: ", new_token_bytes_list)</span></span><br><span class="line">        </span><br><span class="line">        new_token_ids_list = [vocab_reversed[i] <span class="keyword">for</span> i <span class="keyword">in</span> new_token_bytes_list]</span><br><span class="line">        <span class="comment"># print(new_token_ids_list)</span></span><br><span class="line">        <span class="keyword">return</span> new_token_ids_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_iterable</span>(<span class="params">self, iterable: Iterable[<span class="built_in">str</span>]</span>) -&gt; Iterator[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Given an iterable of</span></span><br><span class="line"><span class="string">        strings (e.g., a Python file handle), return a generator that lazily yields token IDs. This is</span></span><br><span class="line"><span class="string">        required for memory-eï¬€icient tokenization of large files that we cannot directly load into memory</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> iterable:</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="variable language_">self</span>.encode(line):</span><br><span class="line">                <span class="keyword">yield</span> idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, ids: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># print("ids:", ids)</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="comment"># print(type(list(self.vocab.keys())[0])) # int</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> ids:</span><br><span class="line">            <span class="comment"># print(self.vocab[id].decode("utf-8"))</span></span><br><span class="line">            <span class="comment"># print(type(self.vocab.get(id, None)))</span></span><br><span class="line">            result.extend(<span class="variable language_">self</span>.vocab.get(<span class="built_in">id</span>, <span class="literal">None</span>))</span><br><span class="line">        <span class="comment"># print("decode result:", result)</span></span><br><span class="line">        result = <span class="built_in">bytes</span>(result).decode(<span class="string">"utf-8"</span>, errors=<span class="string">"replace"</span>)</span><br><span class="line">        <span class="comment"># print("decode result:", result)</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure>

    </div>

    
    
    

    <footer class="post-footer"><script src="//sdk.jinrishici.com/v2/browser/jinrishici.js"></script>
<script>
  jinrishici.load((result) => {
    let jrsc = document.getElementById('jrsc');
    const data = result.data;
    let author = data.origin.author;
    let title = 'ã€Š' + data.origin.title + 'ã€‹';
    let content = data.content.substr(0, data.content.length - 1);
    let dynasty = data.origin.dynasty.substr(0, data.origin.dynasty.length - 1);
    jrsc.innerText = content + ' @ ' + dynasty + 'Â·' + author + title;
  });
</script>
<div style="text-align: center"><span id="jrsc" >æ­£åœ¨åŠ è½½ä»Šæ—¥è¯—è¯....</span></div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>åŸä½œè€…ï¼š </strong>æ¿¬å“²
  </li>
  <li class="post-copyright-link">
      <strong>æœ¬æ–‡é“¾æ¥ï¼š</strong>
      <a href="https://www.junzhe.top/posts/cs336-bpe-tokenizer.html" title="cs336-bpe åˆ†è¯å™¨">https://www.junzhe.top/posts/cs336-bpe-tokenizer.html</a>
  </li>
  <li class="post-copyright-license">
      <strong>ç‰ˆæƒå£°æ˜ï¼š </strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ï¼
  </li>
</ul>
</div>

          <div class="followme">
  <span>æ¬¢è¿å…³æ³¨æˆ‘çš„å…¶å®ƒå‘å¸ƒæ¸ é“</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
      </div>
  </div>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/56761.html" rel="prev" title="å¬å›é˜¶æ®µè´Ÿé‡‡æ ·">
                  <i class="fa fa-angle-left"></i> å¬å›é˜¶æ®µè´Ÿé‡‡æ ·
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    
  
  <div class="comments giscus-container">
  </div>
  
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2024 â€“ 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">æ¿¬å“²</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="ç«™ç‚¹æ€»å­—æ•°">76k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="ç«™ç‚¹é˜…è¯»æ—¶é•¿">1:09</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="æ€»è®¿å®¢é‡">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="æ€»è®¿é—®é‡">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> å¼ºåŠ›é©±åŠ¨
  </div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="è¿”å›é¡¶éƒ¨">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>
<script class="next-config" data-name="giscus" type="application/json">{"enable":true,"repo":"DreamKongcheng/DreamKongcheng.github.io","repo_id":"R_kgDOM33FfA","category":"General","category_id":"DIC_kwDOM33FfM4CqJQz","mapping":"pathname","strict":0,"reactions_enabled":1,"emit_metadata":0,"theme":"light","lang":"zh-CN","crossorigin":"anonymous","input_position":"bottom","loading":"lazy"}</script>

<script>
document.addEventListener('page:loaded', () => {
  if (!CONFIG.page.comments) return;

  NexT.utils.loadComments('.giscus-container')
    .then(() => NexT.utils.getScript('https://giscus.app/client.js', {
      attributes: {
        async                   : true,
        crossOrigin             : 'anonymous',
        'data-repo'             : CONFIG.giscus.repo,
        'data-repo-id'          : CONFIG.giscus.repo_id,
        'data-category'         : CONFIG.giscus.category,
        'data-category-id'      : CONFIG.giscus.category_id,
        'data-mapping'          : CONFIG.giscus.mapping,
        'data-strict'           : CONFIG.giscus.strict,
        'data-reactions-enabled': CONFIG.giscus.reactions_enabled,
        'data-emit-metadata'    : CONFIG.giscus.emit_metadata,
        'data-theme'            : CONFIG.giscus.theme,
        'data-lang'             : CONFIG.giscus.lang,
        'data-input-position'   : CONFIG.giscus.input_position,
        'data-loading'          : CONFIG.giscus.loading
      },
      parentNode: document.querySelector('.giscus-container')
    }));
});
</script>

</body>
</html>
